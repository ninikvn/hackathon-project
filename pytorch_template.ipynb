{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninikvn/hackathon-project/blob/main/pytorch_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop: Pytorch and Jax\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this workshop we are going to explore two other libraries that are commonly used for Deep Learning, Pytorch and Jax.\n",
        "\n",
        "We will start by looking at Pytorch which is on the whole very similar to Tensorflow. We will highlight some of the differences and advantages of each then talk about some of the extras that come with Pytorch.\n",
        "\n",
        "Jax on the other hand is not explicitly a deep learning library, but rather, is a *scientific computing library with auto-differientation.* The key to Jax is that it is fast, **really fast.** Jax is more similar to a super numpy than a Tensorflow or PyTorch\n",
        "\n",
        "___\n",
        "## Tensorflow v. Pytorch\n",
        "\n",
        "Let's start out by taking a look at some familar Tensorflow code, then its PyTorch equivalent"
      ],
      "metadata": {
        "id": "mf69kULyt_nE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJRO0YRWtsVZ"
      },
      "outputs": [],
      "source": [
        "# Start by importing all the libraries we need\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# DataLoaders and Datasets allow us to easily preprocess data, batch, etc.\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# torchvision has some preloaded datasets and image transformations\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Standard imports we will use\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load and preprocess our data using Tensorflow"
      ],
      "metadata": {
        "id": "M42aXu8T9s0a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS7uL_nnv3u9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9373bb50-a5b6-4d43-a481-28e857d39756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Loading in dataset; split into training (X0,Y0) and testing (X1,Y1)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_inputs, train_labels_clean), (test_inputs, test_labels_clean) = mnist.load_data()\n",
        "assert train_inputs.shape == (60000, 28, 28) #width and height of image\n",
        "assert test_inputs.shape == (10000, 28, 28)\n",
        "assert train_labels_clean.shape == (60000,) #just labels, digits 0-9\n",
        "assert test_labels_clean.shape == (10000,)\n",
        "\n",
        "# Normalize inputs to [0, 1] and make sure they're float32\n",
        "X0 = (train_inputs / 255.).astype(np.float32)\n",
        "X1 = (test_inputs / 255.).astype(np.float32)\n",
        "\n",
        "# Make labels one hot vectors\n",
        "def one_hot(arr, num_classes):\n",
        "  \"\"\"Convert array to a one-hot matrix\n",
        "      Hint: We can use np.eye and index by our array\"\"\"\n",
        "  return np.eye(num_classes)[arr]\n",
        "\n",
        "train_labels = one_hot(train_labels_clean, 10).astype(np.float32)\n",
        "test_labels = one_hot(test_labels_clean, 10).astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code creates and trains our Tensorflow model, it should look familiar to you if you've completed the mini-project"
      ],
      "metadata": {
        "id": "o6mXambn_dhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM_Fqz95v8Ww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "da3b66bb-b65d-4d60-e832-63669d7ac949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:05<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 0: 0.5875999927520752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 69.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 1: 0.6908000111579895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 71.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 2: 0.7135000228881836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 71.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 3: 0.7265999913215637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 69.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 4: 0.7336999773979187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 69.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 5: 0.7631000280380249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 70.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 6: 0.8090000152587891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 68.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 7: 0.817300021648407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 66.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 8: 0.8228999972343445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 68.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 9: 0.9045000076293945\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m200,960\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">200,960</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m235,146\u001b[0m (918.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,146</span> (918.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m235,146\u001b[0m (918.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">235,146</span> (918.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"\n",
        "    The model class inherits from tf.keras.Model.\n",
        "    It stores the trainable weights as attributes.\n",
        "      Hint: Using Dense layers with output size 256, 128, 10 should work well\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__(**kwargs)\n",
        "\n",
        "    self.layer1 = tf.keras.layers.Dense(256, activation=\"relu\")\n",
        "    self.layer2 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "    self.layer3 = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    Forward pass, predicts labels given an input image using fully connected layers\n",
        "    :return: the probabilites of each label\n",
        "    \"\"\"\n",
        "\n",
        "    layer1Output = self.layer1(inputs)\n",
        "    layer2Output = self.layer2(layer1Output)\n",
        "    prbs = self.layer3(layer2Output)\n",
        "    return prbs\n",
        "\n",
        "  def loss_fn(self, predictions, labels):\n",
        "    \"\"\"\n",
        "    Calculates the model loss\n",
        "    :return: the loss of the model as a tensor\n",
        "    \"\"\"\n",
        "    nll_comps = -labels * tf.math.log(tf.clip_by_value(predictions,1e-10,1.0))\n",
        "    return tf.reduce_mean(tf.reduce_sum(nll_comps, axis=[1]))\n",
        "\n",
        "  def accuracy(self, predictions, labels):\n",
        "    \"\"\"\n",
        "    Calculates the model accuracy\n",
        "    :return: the accuracy of the model as a tensor\n",
        "    \"\"\"\n",
        "    pred_classes = tf.argmax(predictions, 1)\n",
        "    true_classes = tf.argmax(labels, 1)\n",
        "    correct_prediction = tf.equal(pred_classes, true_classes)\n",
        "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "## END TODO\n",
        "################################################################################\n",
        "\n",
        "# Instantiate our model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Choosing our optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "# Loop through training steps\n",
        "epochs = 10\n",
        "batch_size = 1024\n",
        "train_steps = len(train_inputs) // batch_size\n",
        "for i in range(epochs):\n",
        "  for j in tqdm(range(train_steps)):\n",
        "    image = tf.reshape(train_inputs[j*batch_size:(j+1)*batch_size], (batch_size,-1))\n",
        "    label = tf.reshape(train_labels[j*batch_size:(j+1)*batch_size], (batch_size,-1))\n",
        "    # Implement backprop:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = model(image) # this calls the call function conveniently\n",
        "      label = tf.cast(label, tf.float32)\n",
        "      loss = model.loss_fn(y_pred, label)\n",
        "\n",
        "    # The keras Model class has the computed property trainable_variables to conveniently\n",
        "    # return all the trainable variables you'd want to adjust based on the gradients\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  test_acc = model.accuracy(model(tf.reshape(test_inputs, (-1, 28*28))), test_labels)\n",
        "  print(f\"Accuracy on testing set after epoch {i}: {test_acc}\")\n",
        "\n",
        "print()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to look at the same code but using PyTorch instead of Tensforflow.\n",
        "\n",
        "First, let's take a look at loading and preprocessing data with PyTorch"
      ],
      "metadata": {
        "id": "zV4SbefZAStB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pytorch has it's own methods of loading and batching data, which you can see below\n",
        "\"\"\" Ordinarily the commented code below is all you need to load the MNIST data.\n",
        "      For some reason, Yann Lecun.com is timing out so we instead make our own\n",
        "      dataset to save time.\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
        "                               transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
        "                              transform=transforms.ToTensor())\n",
        "\"\"\"\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "  \"\"\"\n",
        "    Every Pytorch Dataset needs an __init__, __len__, and __getitem__\n",
        "    These methods are used to get and batch the data using a DataLoader later\n",
        "  \"\"\"\n",
        "  def __init__(self, images, labels):\n",
        "    self.images = torch.Tensor(images)\n",
        "    self.labels = torch.Tensor(labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "train_dataset = MNISTDataset(train_inputs, train_labels)\n",
        "test_dataset = MNISTDataset(test_inputs, test_labels)\n",
        "\n",
        "\n",
        "# dataloaders are an easy way to batch and shuffle datasets\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "Goowmhe18CdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our data, let's create and train a PyTorch Model! It looks very similar to Tensorflow, with some small differences in naming conventions, etc."
      ],
      "metadata": {
        "id": "OlTYc53aA2Hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507e4ee5-9bef-4764-cc8b-ef7f6af42f54",
        "id": "9SYjEFmIviMw"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 40.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 0: 0.9364855885505676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 53.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 1: 0.9549247026443481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 50.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 2: 0.9593949317932129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 52.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 3: 0.9654256701469421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 43.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 4: 0.9686263799667358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 52.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 5: 0.9700155258178711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 51.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 6: 0.9710897207260132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 49.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 7: 0.9715701341629028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:02, 27.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 8: 0.9713448286056519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "59it [00:01, 39.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on testing set after epoch 9: 0.9723971486091614\n",
            "\n",
            "Model(\n",
            "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (layer3): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"\n",
        "    The model class inherits from tf.keras.Model.\n",
        "    It stores the trainable weights as attributes.\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__(**kwargs)\n",
        "\n",
        "    # Initialize our torch.nn.Linear layers again we use 256, 128, 10\n",
        "    self.layer1 = torch.nn.Linear(784, 256)\n",
        "    self.layer2 = torch.nn.Linear(256, 128)\n",
        "    self.layer3 = torch.nn.Linear(128, 10)\n",
        "\n",
        "    # PyTorch Linear Layers don't let you nicely initialize an activation function\n",
        "    #   line TF does so we need to create these explicitly\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"\n",
        "    Forward pass, predicts labels given an input image using fully connected layers\n",
        "    :return: the probabilites of each label\n",
        "    \"\"\"\n",
        "\n",
        "    out1 = self.layer1(inputs)\n",
        "    out1 = self.relu(out1)\n",
        "    out2 = self.layer2(out1)\n",
        "    out2 = self.relu(out2)\n",
        "    out3 = self.layer3(out2)\n",
        "    prbs = self.softmax(out3)\n",
        "    return prbs\n",
        "\n",
        "  def loss(self, predictions, labels):\n",
        "    \"\"\"\n",
        "    Calculates the model loss\n",
        "    :return: the loss of the model as a tensor\n",
        "    \"\"\"\n",
        "    nll_comps = -labels * torch.log(torch.clip(predictions,1e-10,1.0))\n",
        "    return torch.mean(torch.sum(nll_comps, axis=[1]))\n",
        "\n",
        "  def accuracy(self, predictions, labels):\n",
        "    \"\"\"\n",
        "    Calculates the model accuracy\n",
        "    :return: the accuracy of the model as a tensor\n",
        "    \"\"\"\n",
        "    pred_classes = torch.argmax(predictions, 1)\n",
        "    true_classes = torch.argmax(labels, 1)\n",
        "    correct_prediction = torch.eq(pred_classes, true_classes)\n",
        "    return torch.mean(torch.Tensor(correct_prediction).to(torch.float32))\n",
        "\n",
        "## END TODO\n",
        "################################################################################\n",
        "\n",
        "# Instantiate our model\n",
        "model = Model()\n",
        "\n",
        "# Create our optimizer, notice that the parameters are passed into the init.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loop through training steps\n",
        "epochs = 10\n",
        "\n",
        "for j in range(epochs):\n",
        "  for batch_idx, (input, label) in tqdm(enumerate(train_loader)):\n",
        "    # There isn't a \"GradientTape\" context manager for torch\n",
        "    #   Instead, torch Tensors have a backward method which backpropagates\n",
        "    #   automatically. We will talk a little about some of these differences later\n",
        "\n",
        "    input = torch.reshape(input, (len(input),-1))\n",
        "    y_pred = model(input) # this calls the call function conveniently\n",
        "    loss = model.loss(y_pred, label) # compute the loss\n",
        "    loss.backward() # compute and assign the gradients via backprop\n",
        "    optimizer.step() # update the parameters\n",
        "    optimizer.zero_grad() # reset the stored gradients for each of the parameters (can also move this above the line that starts with input)\n",
        "\n",
        "  test_acc = 0\n",
        "  for batch_idx, (input, label) in enumerate(test_loader):\n",
        "    input = torch.reshape(input, (len(input),-1))\n",
        "    test_acc += model.accuracy(model(input), label)\n",
        "  print(f\"Accuracy on testing set after epoch {j}: {test_acc/len(test_loader)}\")\n",
        "print()\n",
        "print(model)\n",
        "\n",
        "# Different optimizer used here; tf basic but here Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's zoom into some of the differences between these two Model and training methods.\n",
        "\n",
        "For one, in Tensorflow we subclass\n",
        "\n",
        "```\n",
        "class Model(tf.keras.Model)\n",
        "```\n",
        "and in PyTorch we have\n",
        "```\n",
        "class Model(torch.nn.Module)\n",
        "```\n",
        "\n",
        "Also in PyTorch, Tensorflow's ```call``` method is known as the ```forward``` method, but they both complete one forward pass.\n",
        "\n",
        "Dense/Linear layers are initialized with ```torch.nn.Linear(in_dimension, out_dimension)```\n",
        "instead of ```tf.keras.layers.Dense(out_dimension)```. In torch you have to specify the in_dimension, whereas tensorflow figures this out after you make your first call to the model. This small difference makes PyTorch typically faster for the first handful of iterations. That said, Tensorflow's backpropagation is generally faster so it catches up as the number of iterations grows.\n",
        "\n",
        "Some of the other methods have small name changes, like ```tf.clip_by_value -> torch.clip.```\n",
        "\n",
        "Perhaps the biggest difference between these two frameworks is how backpropagation is done. Notice that in Tensorflow we use\n",
        "```\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y_pred = model(image) # this calls the call function conveniently\n",
        "  loss = model.loss(y_pred, label)\n",
        "\n",
        "gradients = tape.gradient(loss, model.trainable_variables)\n",
        "optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "```\n",
        "\n",
        "But in PyTorch we have\n",
        "\n",
        "```\n",
        "# Reset the gradient of the trainable params to 0\n",
        "optimizer.zero_grad()\n",
        "y_pred = model(image) # this calls the call function conveniently\n",
        "loss = model.loss(y_pred, label)\n",
        "\n",
        "# Compute the gradients\n",
        "loss.backward()\n",
        "# Apply the gradients to the trainable parameters\n",
        "optimizer.step()\n",
        "```\n",
        "Additionally, in PyTorch we initialize our optimizer *with the trainable parameters* instead of passing the trainable parameters into every backward pass like we do in Tensorflow."
      ],
      "metadata": {
        "id": "i3KtuQ-i0Wkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Add-Ons and Niceties\n",
        "\n",
        "Pytorch has a handful of additional libraries that contain useful content like **torchvision and torchaudio.** These libraries contain tons of helpful content to streamline data preprocessing, transformations and postprocessing. For more information, you can refer to [torchvision](https://pytorch.org/vision/0.20/) and [torchaudio](https://pytorch.org/audio/stable/index.html)."
      ],
      "metadata": {
        "id": "uM4DpSD52nsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computational Graph\n",
        "Static Graph (e.g., TensorFlow 1.x): The computation graph is defined before execution. Think of it as writing down a complete recipe first, and then following it to cook. You need to compile and run the graph using a session. Good for optimization, but harder to debug and less flexible for dynamic tasks.\n",
        "\n",
        "Dynamic Graph (e.g., PyTorch): The computation graph is built on the fly during execution. Imagine cooking step by step, deciding what to do at each step. You can directly use Python constructs like loops and print statements. More intuitive and flexible, great for debugging and dynamic tasks.\n",
        "### Dynamic Graph\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0PHb89XM__b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing torch\n",
        "import torch\n",
        "\n",
        "# Initializing input tensors\n",
        "a = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Computing the output\n",
        "c = a * b\n",
        "print('value of c: ', c)\n",
        "\n",
        "# Displaying the outputs\n",
        "print(f'c_out = {c}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7hc6HzxC2Zl",
        "outputId": "f6bc7c3c-7ca4-4f7e-b0f5-902c7b156208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value of c:  tensor(2., grad_fn=<MulBackward0>)\n",
            "c_out = 2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Static Graph\n"
      ],
      "metadata": {
        "id": "kBGtvYfuCxCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing tensorflow version 1\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Initializing placeholder variables of\n",
        "# the graph\n",
        "a = tf.placeholder(tf.float32)\n",
        "b = tf.placeholder(tf.float32)\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  # Defining the operation\n",
        "  c = tf.multiply(a, b)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  # Running the graph\n",
        "  c_out = sess.run(c, feed_dict={a: 1.0, b: 2.0})\n",
        "\n",
        "print(c_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRGO0Z_9Cy-3",
        "outputId": "599aaa46-dd05-411d-9604-769bdbf54940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a static computation graph, you cannot use Python's print function to retrieve or display values within the graph during execution. This is because the graph defines operations symbolically, and values are only computed during a session run.\n",
        "\n",
        "If you try to use print to inspect tensors directly in the graph, it will display the symbolic representation of the tensor (e.g., Tensor(\"add:0\", shape=(), dtype=float32)) instead of the actual values."
      ],
      "metadata": {
        "id": "yfDI2bYIMNNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()  # Use static graph mode\n",
        "\n",
        "# Define placeholders\n",
        "x = tf.placeholder(tf.float32, shape=(None,))\n",
        "y = tf.placeholder(tf.float32, shape=(None,))\n",
        "\n",
        "# Example computation\n",
        "z = x + y\n",
        "\n",
        "# Add a print operation (does not modify the value of z)\n",
        "print(\"Information from print: \", z)\n",
        "z_print = tf.print('tf.Print: ', z)\n",
        "\n",
        "# Create a session to execute the graph\n",
        "with tf.Session() as sess:\n",
        "    result, z_print = sess.run([z, z_print], feed_dict={x: [1.0, 2.0], y: [3.0, 4.0]})\n",
        "    print(\"Result:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s9vM2oTMCRR",
        "outputId": "2b92c03a-fadd-42b7-9a81-c7e6ab50202a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Information from print:  Tensor(\"add:0\", shape=(?,), dtype=float32)\n",
            "tf.Print:  [4 6]\n",
            "Result: [4. 6.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a static computation graph, **for loops** work differently compared to dynamic graphs. You cannot directly use Python's native for loops or if conditions when constructing the graph. Doing so may lead to unexpected behavior or errors because Python constructs are not part of the computation graph and won't execute as intended during graph execution.\n",
        "\n",
        "Instead, if you're using TensorFlow 1.x, you need to use TensorFlow's built-in control flow operations, such as `tf.while_loop` for loops and tf.cond for conditions. These operations ensure that the control flow is properly represented within the graph."
      ],
      "metadata": {
        "id": "7Xz4BPBxMOAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "# Example loop to compute the sum of numbers from 0 to 9\n",
        "n = tf.constant(10)\n",
        "\n",
        "# Loop variables\n",
        "i = tf.constant(0)  # Initial index\n",
        "total = tf.constant(0)  # Accumulator\n",
        "\n",
        "# Define the loop condition and body\n",
        "def condition(i, total):\n",
        "    return i < n\n",
        "\n",
        "def body(i, total):\n",
        "    total += i\n",
        "    i += 1\n",
        "    return i, total\n",
        "\n",
        "# Use tf.while_loop\n",
        "final_i, final_total = tf.while_loop(condition, body, [i, total])\n",
        "\n",
        "# Run the graph\n",
        "with tf.Session() as sess:\n",
        "    result = sess.run(final_total)\n",
        "    print(\"Sum of numbers from 0 to 9:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffDI6e4UMOXn",
        "outputId": "a3c3af0d-c467-4638-f4ea-5d68a8bc6a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of numbers from 0 to 9: 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "\n",
        "# Jax\n",
        "\n",
        "JAX is a library for array-oriented numerical computation (à la NumPy), with automatic differentiation and JIT compilation to enable high-performance machine learning research.\n",
        "\n",
        "By default, you can just use jax.numpy with all your favorite numpy operations as typical. Let's take a look at some examples of this."
      ],
      "metadata": {
        "id": "RnOx5fkh3bVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "iwpff_J_5XlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "  return jnp.where(x > 0, x, alpha * x)\n",
        "\n",
        "# We want an array of numbers [-2, -1, 0, 1, 2]. In numpy we have arange,\n",
        "#    how about in jnp?\n",
        "x = jnp.arange(-2, 3)\n",
        "print(leaky_relu(x, alpha=0.1))"
      ],
      "metadata": {
        "id": "4b3YBFtdogv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d686583c-7817-4992-d5f7-4ac5c9ef810a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.2 -0.1  0.   1.   2. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, we can use standard numpy methods like where and arange in jax exactly as we would have in numpy.\n",
        "\n",
        "One of Jax's most powerful features is \"just-in-time\" (jit) compilation. This feature allows jax to precompile the operations for python methods to compute the results very quickly. Let's take a look at how numpy's speed compares to jax and jax+jit."
      ],
      "metadata": {
        "id": "XSuw2N70pANr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.random.normal(0,1,1_000_000)\n",
        "\n",
        "def numpy_leaky_relu(x, alpha=0.01):\n",
        "  return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "%timeit numpy_leaky_relu(x, alpha=0.1)"
      ],
      "metadata": {
        "id": "cA_F7vVyqYjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5790779a-20b5-4024-c174-1d16056b0d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.43 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "\n",
        "# initialize a pseudo-random number generation key.\n",
        "key = random.key(1470)\n",
        "\n",
        "# create a million standard normal random numbers\n",
        "x = random.normal(key, (1_000_000))\n",
        "\n",
        "# time how long it takes to run this function on our array\n",
        "%timeit leaky_relu(x, alpha=0.1).block_until_ready()"
      ],
      "metadata": {
        "id": "HPiiqclZpfd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb1008a-7bec-49ce-a3ba-1923fb044396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "562 µs ± 18.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import jit\n",
        "\n",
        "jit_leaky_relu = jit(leaky_relu)\n",
        "\n",
        "# we have to use the function once to let it compile with jit\n",
        "_ = jit_leaky_relu(x, alpha=0.1)\n",
        "\n",
        "%timeit jit_leaky_relu(x, alpha=0.1).block_until_ready()"
      ],
      "metadata": {
        "id": "QSBnUNZUqrtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d6d4f0-eace-41b1-adf5-de28027cbc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "243 µs ± 10.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that in this case, base jax is about 10x faster than numpy and using jit cuts standard jax's time in half again! This is a pretty remarkable speed-up but it's only a slice of what jax can do.\n",
        "\n",
        "Jax isn't always faster than numpy, but will generally outperform numpy in situations where using a GPU is faster than CPU (like deep learning). You can read more about the comparison [here](https://jax.readthedocs.io/en/latest/faq.html#is-jax-faster-than-numpy).\n",
        "\n",
        "Now let's see how we can differentiate functions in jax."
      ],
      "metadata": {
        "id": "W20Bw7Pcq50w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad #computes gradients\n",
        "\n",
        "def sum_of_squares(x):\n",
        "  # in numpy we can use np.sum and np.square\n",
        "  #   How about in jnp?\n",
        "  return jnp.sum(jnp.square(x))\n",
        "\n",
        "input = jnp.arange(5.)\n",
        "\n",
        "# Here we are going to create a new function that computes the gradient\n",
        "#    of the sum_of_squares function\n",
        "grad_sum_squares = grad(sum_of_squares)\n",
        "\n",
        "# output the value of the function\n",
        "print(sum_of_squares(input))\n",
        "\n",
        "# output the gradient at these points\n",
        "print(grad_sum_squares(input))"
      ],
      "metadata": {
        "id": "jZ-Y_jyQrMXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb21ffe0-9bdf-4983-a62a-e604fd3ed0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30.0\n",
            "[0. 2. 4. 6. 8.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like that, we can differentiate functions with Jax!\n",
        "\n",
        "Now we have everything we need to train a model in Jax like the one we had using PyTorch and Tensorflow. We should note here that there is a pretty significant difference in how Jax goes about training a network as compared to Tensorflow and PyTorch. It's a good exercise to think about all the steps that go into training a model, ignorant of what framework you use.\n",
        "\n",
        "Let's start by intializing the model weights and hyperparamters."
      ],
      "metadata": {
        "id": "I3gdT2YhsGMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function to randomly initialize weights and biases\n",
        "# for a dense neural network layer\n",
        "def random_layer_params(in_size, out_size, key, scale=1e-2):\n",
        "  w_key, b_key = random.split(key)\n",
        "\n",
        "  # notice our weights are out_size x in_size, that's just for matmul shaping.\n",
        "  #   This initialization is called the He/Kaiming normal initialization\n",
        "  scale = (jnp.sqrt(2/jnp.sum(jnp.array([in_size, out_size]))))\n",
        "  return scale*random.normal(w_key, (out_size, in_size)), scale * random.normal(b_key, (out_size,))\n",
        "\n",
        "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(sizes, key):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "layer_sizes = [784, 256, 128, 10]\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 1024\n",
        "n_classes = 10\n",
        "params = init_network_params(layer_sizes, random.key(0))"
      ],
      "metadata": {
        "id": "hnWN-QX1vxpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "def predict(params, image):\n",
        "  # per-example predictions\n",
        "  activations = image\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = jnp.dot(w, activations) + b # we need the dot product and the bias addition\n",
        "    activations = jit_leaky_relu(outputs)\n",
        "\n",
        "  final_w, final_b = params[-1]\n",
        "  logits = jnp.dot(final_w, activations) + final_b # what goes here?\n",
        "  return logits - logsumexp(logits) # Nothing more than log(softmax) here"
      ],
      "metadata": {
        "id": "pVMWpd4Mwy93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This works on single examples\n",
        "random_flattened_image = random.normal(random.key(1), (28 * 28,))\n",
        "preds = predict(params, random_flattened_image)\n",
        "print(preds.shape)\n",
        "\n",
        "# But when we try with batch it breaks\n",
        "random_flattened_images = random.normal(random.key(1), (10, 28 * 28))\n",
        "try:\n",
        "  preds = predict(params, random_flattened_images)\n",
        "except TypeError:\n",
        "  print('Invalid shapes!')"
      ],
      "metadata": {
        "id": "Ty5FXouTzNvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc76e8c-0afc-4758-ecd4-ef90b7d8817a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,)\n",
            "Invalid shapes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above example illustrates an important principle of Jax. You write the code to handle one example, then use Jax operations like vmap to generalize to the batched case. It takes some getting used to, but in many cases becomes a more natural coding experience.\n",
        "\n",
        "Let's see how we can generalize our pipeline to handle batched inputs using vmap."
      ],
      "metadata": {
        "id": "v5VJa9VxzmkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import vmap\n",
        "\n",
        "# Make a batched version of the `predict` function\n",
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "# `batched_predict` has the same call signature as `predict`\n",
        "batched_preds = batched_predict(params, random_flattened_images)\n",
        "print(batched_preds.shape)"
      ],
      "metadata": {
        "id": "ebTgX59qz8cI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7bf9d1-7250-402c-a71d-a0044fc91ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can handle batches to predict our outputs. We have almost everything we need to train our network! Let's finish up with some utility functions and speed up the process using jit"
      ],
      "metadata": {
        "id": "wkDCyvje0Is_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(x, k, dtype=jnp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  # In numpy we used eye + indexing, what should we do in jnp?\n",
        "  return jnp.eye(k, dtype=dtype)[x]\n",
        "\n",
        "def accuracy(params, images, targets):\n",
        "  \"\"\"Compute the accuracy for a set of images and targets.\"\"\"\n",
        "  target_class = jnp.argmax(targets, axis=1)\n",
        "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
        "  return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "def loss(params, images, targets):\n",
        "  \"\"\"Compute the multi-class cross-entropy loss.\"\"\"\n",
        "\n",
        "  # here we want to get our predictions then compute\n",
        "  preds = batched_predict(params, images)\n",
        "\n",
        "  # Now we want to compute Categorical Cross Entropy then take the mean\n",
        "  #    any guesses how we can compute that easily?\n",
        "  #    hint: remember that we output log(prob) for each prediction\n",
        "  #    hint: Categorical Cross Entropy = -sum(log(prob)*true_value)\n",
        "  loss_value = -jnp.sum(preds*targets) # here\n",
        "  return loss_value\n",
        "\n",
        "@jit\n",
        "def update(params, x, y):\n",
        "  # This is standard SGD, nothing fancy here\n",
        "  grads = grad(loss)(params, x, y) # Compute the gradient\n",
        "  return [(w - learning_rate * dw, b - learning_rate * db)\n",
        "          for (w, b), (dw, db) in zip(params, grads)] # update the weights"
      ],
      "metadata": {
        "id": "9cylzcQR0Wjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Ensure TF does not see GPU and grab all GPU memory.\n",
        "#tf.config.set_visible_devices([], device_type='GPU')\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "\n",
        "# Fetch full datasets for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
        "mnist_data = tfds.as_numpy(mnist_data)\n",
        "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "num_labels = info.features['label'].num_classes\n",
        "h, w, c = info.features['image'].shape\n",
        "num_pixels = h * w * c\n",
        "\n",
        "# Full train set\n",
        "jax_train_images, jax_train_labels = train_data['image'], train_data['label']\n",
        "jax_train_images = jnp.reshape(jax_train_images, (len(jax_train_labels), num_pixels))\n",
        "jax_train_labels = one_hot(jax_train_labels, num_labels)\n",
        "\n",
        "# Full test set\n",
        "jax_test_images, jax_test_labels = test_data['image'], test_data['label']\n",
        "jax_test_images = jnp.reshape(jax_test_images, (len(jax_test_images), num_pixels))\n",
        "jax_test_labels = one_hot(jax_test_labels, num_labels)\n"
      ],
      "metadata": {
        "id": "w6QKdOz51tAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train out model and see how it does!"
      ],
      "metadata": {
        "id": "sRPu98KT2mvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=10\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  num_batches = len(jax_train_images)//batch_size\n",
        "  start_time = time.time()\n",
        "  for i in tqdm(range(num_batches)):\n",
        "    # Get our batches\n",
        "    x = jax_train_images[i*batch_size:(i+1)*batch_size]\n",
        "    y = jax_train_labels[i*batch_size:(i+1)*batch_size]\n",
        "    # update the parameters!\n",
        "    params = update(params, x, y)\n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  train_acc = accuracy(params, jax_train_images, jax_train_labels)\n",
        "  test_acc = accuracy(params, jax_test_images, jax_test_labels)\n",
        "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "  print(\"Training set accuracy {}\".format(train_acc))\n",
        "  print(\"Test set accuracy {}\".format(test_acc))"
      ],
      "metadata": {
        "id": "BdL-1kEM2mKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6728c775-d25c-4596-ebb0-b53a2a4d6faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1009.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 in 0.06 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1000.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 in 0.06 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1179.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 in 0.06 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1179.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 in 0.05 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1039.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 in 0.06 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1118.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 in 0.06 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 924.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 in 0.07 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1142.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 in 0.06 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 949.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 in 0.07 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 1126.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 in 0.06 sec\n",
            "Training set accuracy 0.09871666878461838\n",
            "Test set accuracy 0.09799999743700027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf0OfBO34JId"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}