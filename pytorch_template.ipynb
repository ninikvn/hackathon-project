{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninikvn/hackathon-project/blob/main/pytorch_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Goowmhe18CdD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pytorch has it's own methods of loading and batching data, which you can see below\n",
        "\"\"\" Ordinarily the commented code below is all you need to load the MNIST data.\n",
        "      For some reason, Yann Lecun.com is timing out so we instead make our own\n",
        "      dataset to save time.\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True,\n",
        "                               transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
        "                              transform=transforms.ToTensor())\n",
        "\"\"\"\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "  \"\"\"\n",
        "    Every Pytorch Dataset needs an __init__, __len__, and __getitem__\n",
        "    These methods are used to get and batch the data using a DataLoader later\n",
        "  \"\"\"\n",
        "  def __init__(self, images, labels):\n",
        "    self.images = torch.Tensor(images)\n",
        "    self.labels = torch.Tensor(labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.images[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "train_dataset = MNISTDataset(train_inputs, train_labels)\n",
        "test_dataset = MNISTDataset(test_inputs, test_labels)\n",
        "\n",
        "\n",
        "# dataloaders are an easy way to batch and shuffle datasets\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlTYc53aA2Hs"
      },
      "source": [
        "Now that we have our data, let's create and train a PyTorch Model! It looks very similar to Tensorflow, with some small differences in naming conventions, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SYjEFmIviMw",
        "outputId": "507e4ee5-9bef-4764-cc8b-ef7f6af42f54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 40.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 0: 0.9364855885505676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 53.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 1: 0.9549247026443481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 50.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 2: 0.9593949317932129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 52.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 3: 0.9654256701469421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 43.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 4: 0.9686263799667358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 52.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 5: 0.9700155258178711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 51.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 6: 0.9710897207260132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 49.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 7: 0.9715701341629028\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:02, 27.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 8: 0.9713448286056519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "59it [00:01, 39.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on testing set after epoch 9: 0.9723971486091614\n",
            "\n",
            "Model(\n",
            "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (layer3): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Model(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    \"\"\"\n",
        "    The model class inherits from tf.keras.Model.\n",
        "    It stores the trainable weights as attributes.\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__(**kwargs)\n",
        "\n",
        "    # Initialize our torch.nn.Linear layers again we use 256, 128, 10\n",
        "    self.layer1 = torch.nn.Linear(784, 256)\n",
        "    self.layer2 = torch.nn.Linear(256, 128)\n",
        "    self.layer3 = torch.nn.Linear(128, 10)\n",
        "\n",
        "    # PyTorch Linear Layers don't let you nicely initialize an activation function\n",
        "    #   line TF does so we need to create these explicitly\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "  def forward(self, inputs):\n",
        "    \"\"\"\n",
        "    Forward pass, predicts labels given an input image using fully connected layers\n",
        "    :return: the probabilites of each label\n",
        "    \"\"\"\n",
        "\n",
        "    out1 = self.layer1(inputs)\n",
        "    out1 = self.relu(out1)\n",
        "    out2 = self.layer2(out1)\n",
        "    out2 = self.relu(out2)\n",
        "    out3 = self.layer3(out2)\n",
        "    prbs = self.softmax(out3)\n",
        "    return prbs\n",
        "\n",
        "  def loss(self, predictions, labels):\n",
        "    \"\"\"\n",
        "    Calculates the model loss\n",
        "    :return: the loss of the model as a tensor\n",
        "    \"\"\"\n",
        "    nll_comps = -labels * torch.log(torch.clip(predictions,1e-10,1.0))\n",
        "    return torch.mean(torch.sum(nll_comps, axis=[1]))\n",
        "\n",
        "  def accuracy(self, predictions, labels):\n",
        "    \"\"\"\n",
        "    Calculates the model accuracy\n",
        "    :return: the accuracy of the model as a tensor\n",
        "    \"\"\"\n",
        "    pred_classes = torch.argmax(predictions, 1)\n",
        "    true_classes = torch.argmax(labels, 1)\n",
        "    correct_prediction = torch.eq(pred_classes, true_classes)\n",
        "    return torch.mean(torch.Tensor(correct_prediction).to(torch.float32))\n",
        "\n",
        "## END TODO\n",
        "################################################################################\n",
        "\n",
        "# Instantiate our model\n",
        "model = Model()\n",
        "\n",
        "# Create our optimizer, notice that the parameters are passed into the init.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Loop through training steps\n",
        "epochs = 10\n",
        "\n",
        "for j in range(epochs):\n",
        "  for batch_idx, (input, label) in tqdm(enumerate(train_loader)):\n",
        "    # There isn't a \"GradientTape\" context manager for torch\n",
        "    #   Instead, torch Tensors have a backward method which backpropagates\n",
        "    #   automatically. We will talk a little about some of these differences later\n",
        "\n",
        "    input = torch.reshape(input, (len(input),-1))\n",
        "    y_pred = model(input) # this calls the call function conveniently\n",
        "    loss = model.loss(y_pred, label) # compute the loss\n",
        "    loss.backward() # compute and assign the gradients via backprop\n",
        "    optimizer.step() # update the parameters\n",
        "    optimizer.zero_grad() # reset the stored gradients for each of the parameters (can also move this above the line that starts with input)\n",
        "\n",
        "  test_acc = 0\n",
        "  for batch_idx, (input, label) in enumerate(test_loader):\n",
        "    input = torch.reshape(input, (len(input),-1))\n",
        "    test_acc += model.accuracy(model(input), label)\n",
        "  print(f\"Accuracy on testing set after epoch {j}: {test_acc/len(test_loader)}\")\n",
        "print()\n",
        "print(model)\n",
        "\n",
        "# Different optimizer used here; tf basic but here Adam"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
